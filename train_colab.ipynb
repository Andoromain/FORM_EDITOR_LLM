{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Entra√Ænement Llama 3.2 3B pour G√©n√©rateur de Formulaires\n",
    "\n",
    "Ce notebook permet d'entra√Æner un mod√®le Llama 3.2 3B avec LoRA pour g√©n√©rer des structures de formulaires JSON.\n",
    "\n",
    "**Configuration recommand√©e:**\n",
    "- Runtime: Python 3\n",
    "- Hardware accelerator: GPU (T4, V100, ou A100)\n",
    "- RAM: High-RAM si disponible\n",
    "\n",
    "**Temps estim√©:** 1-3 heures selon le GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üìã √âtape 1: Configuration de l'environnement\n",
    "\n",
    "Installation des biblioth√®ques n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install -q -U transformers datasets accelerate peft bitsandbytes sentencepiece\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# V√©rification du GPU\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Attention: GPU non disponible. L'entra√Ænement sera tr√®s lent.\")\n",
    "    print(\"Allez dans Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdrive"
   },
   "source": [
    "## üíæ √âtape 2: Connexion √† Google Drive\n",
    "\n",
    "Pour sauvegarder le mod√®le entra√Æn√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cr√©er le dossier de sauvegarde\n",
    "!mkdir -p /content/drive/MyDrive/llama3-form-generator\n",
    "\n",
    "print(\"‚úÖ Google Drive mont√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth"
   },
   "source": [
    "## üîê √âtape 3: Authentification Hugging Face\n",
    "\n",
    "Llama 3.2 3B n√©cessite une acceptation de la licence et un token Hugging Face.\n",
    "\n",
    "**Instructions:**\n",
    "1. Allez sur https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "2. Cliquez sur \"Agree and access repository\"\n",
    "3. Cr√©ez un token sur https://huggingface.co/settings/tokens\n",
    "4. Collez-le ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_login"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from getpass import getpass\n",
    "\n",
    "# Entrez votre token Hugging Face\n",
    "token = getpass(\"Entrez votre token Hugging Face: \")\n",
    "login(token=token)\n",
    "\n",
    "print(\"‚úÖ Authentification r√©ussie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## üìÅ √âtape 4: Pr√©paration des donn√©es\n",
    "\n",
    "Deux options:\n",
    "- **Option A**: Uploader votre dataset existant\n",
    "- **Option B**: Cloner le repository GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "# Option A: Upload manuel du dataset\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Uploadez votre fichier training_dataset.jsonl:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(\"‚úÖ Dataset upload√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Option B: Cloner le repository (d√©commentez si vous utilisez cette option)\n",
    "# !git clone https://github.com/VOTRE_USERNAME/FORM_EDITOR_LLM.git\n",
    "# %cd FORM_EDITOR_LLM\n",
    "# !python prepare_dataset.py\n",
    "# !cp training_dataset.jsonl /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [],
   "source": [
    "# V√©rifier le dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "dataset_path = \"/content/training_dataset.jsonl\"\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"‚úÖ Dataset trouv√©: {len(lines)} exemples\")\n",
    "        \n",
    "        # Afficher un exemple\n",
    "        if lines:\n",
    "            example = json.loads(lines[0])\n",
    "            print(\"\\nüìã Exemple:\")\n",
    "            print(f\"Instruction: {example['instruction'][:100]}...\")\n",
    "            print(f\"Output: {example['output'][:100]}...\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset non trouv√©. Veuillez uploader training_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": [
    "## üèãÔ∏è √âtape 5: Entra√Ænement du mod√®le\n",
    "\n",
    "Cette cellule lance l'entra√Ænement avec Llama 3.2 3B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"üöÄ D√©marrage de l'entra√Ænement...\\n\")\n",
    "\n",
    "# Configuration\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "output_dir = \"/content/llama3-form-generator\"\n",
    "\n",
    "# Charger le tokenizer\n",
    "print(\"üì• Chargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Charger le mod√®le\n",
    "print(\"üì• Chargement du mod√®le (cela peut prendre quelques minutes)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Pr√©parer pour LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configuration LoRA\n",
    "print(\"üîß Configuration LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Charger le dataset\n",
    "print(\"\\nüìö Chargement du dataset...\")\n",
    "dataset = load_dataset('json', data_files=\"/content/training_dataset.jsonl\", split='train')\n",
    "print(f\"Dataset size: {len(dataset)} exemples\")\n",
    "\n",
    "# Format de prompt pour Llama 3.2 Instruct\n",
    "def format_and_tokenize(example):\n",
    "    text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Tu es un assistant sp√©cialis√© dans la g√©n√©ration de structures de formulaires JSON.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{example['instruction']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{example['output']}<|eot_id|>\"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Tokenisation du dataset...\")\n",
    "dataset = dataset.map(\n",
    "    format_and_tokenize,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])} | Test: {len(dataset['test'])}\")\n",
    "\n",
    "# Configuration d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "print(\"\\nüèãÔ∏è D√©but de l'entra√Ænement...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Entra√Æner\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## üíæ √âtape 6: Sauvegarde du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le localement\n",
    "print(\"üíæ Sauvegarde du mod√®le...\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le sauvegard√© dans {output_dir}\")\n",
    "\n",
    "# Copier vers Google Drive\n",
    "print(\"\\nüì§ Copie vers Google Drive...\")\n",
    "!cp -r {output_dir} /content/drive/MyDrive/\n",
    "\n",
    "print(\"‚úÖ Mod√®le sauvegard√© sur Google Drive!\")\n",
    "print(f\"üìÅ Chemin: /content/drive/MyDrive/llama3-form-generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test"
   },
   "source": [
    "## üß™ √âtape 7: Test du mod√®le\n",
    "\n",
    "Testons le mod√®le avec un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def test_model(instruction):\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Tu es un assistant sp√©cialis√© dans la g√©n√©ration de structures de formulaires JSON.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraire la r√©ponse\n",
    "    if \"assistant\" in result:\n",
    "        response = result.split(\"assistant\")[1].strip()\n",
    "        return response\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "print(\"üß™ Test du mod√®le:\\n\")\n",
    "instruction = \"Cr√©e un formulaire d'inscription avec nom, pr√©nom, email et t√©l√©phone\"\n",
    "print(f\"Instruction: {instruction}\\n\")\n",
    "\n",
    "result = test_model(instruction)\n",
    "print(f\"R√©sultat:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## üì• √âtape 8: T√©l√©charger le mod√®le (optionnel)\n",
    "\n",
    "Si vous voulez t√©l√©charger le mod√®le localement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_download"
   },
   "outputs": [],
   "source": [
    "# Compresser le mod√®le\n",
    "!zip -r llama3-form-generator.zip {output_dir}\n",
    "\n",
    "# T√©l√©charger\n",
    "from google.colab import files\n",
    "files.download('llama3-form-generator.zip')\n",
    "\n",
    "print(\"‚úÖ T√©l√©chargement lanc√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üìä R√©sum√©\n",
    "\n",
    "### Ce que vous avez fait:\n",
    "1. ‚úÖ Configur√© l'environnement Colab\n",
    "2. ‚úÖ Authentifi√© avec Hugging Face\n",
    "3. ‚úÖ Charg√© le dataset d'entra√Ænement\n",
    "4. ‚úÖ Entra√Æn√© Llama 3.2 3B avec LoRA\n",
    "5. ‚úÖ Sauvegard√© le mod√®le sur Google Drive\n",
    "6. ‚úÖ Test√© le mod√®le\n",
    "\n",
    "### Prochaines √©tapes:\n",
    "- Utilisez le notebook `inference_colab.ipynb` pour g√©n√©rer des formulaires\n",
    "- Le mod√®le est disponible sur votre Drive: `/content/drive/MyDrive/llama3-form-generator`\n",
    "- Vous pouvez affiner l'entra√Ænement en modifiant les hyperparam√®tres\n",
    "\n",
    "### Ressources:\n",
    "- [Documentation Llama 3.2](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)\n",
    "- [Documentation LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
    "- [Documentation Transformers](https://huggingface.co/docs/transformers/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
