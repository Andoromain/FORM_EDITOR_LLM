# üöÄ Guide d'utilisation - Llama 3.2 3B avec Google Colab

Ce guide explique comment entra√Æner et utiliser Llama 3.2 3B pour g√©n√©rer des structures de formulaires JSON, avec support complet pour Google Colab.

## üìã Table des mati√®res

- [Vue d'ensemble](#vue-densemble)
- [Pr√©requis](#pr√©requis)
- [Option 1: Google Colab (Recommand√©)](#option-1-google-colab-recommand√©)
- [Option 2: Environnement local](#option-2-environnement-local)
- [Structure du projet](#structure-du-projet)
- [FAQ et d√©pannage](#faq-et-d√©pannage)

## üéØ Vue d'ensemble

Ce projet vous permet de:
- ‚úÖ Entra√Æner un mod√®le Llama 3.2 3B (fine-tuning avec LoRA)
- ‚úÖ G√©n√©rer des structures de formulaires JSON √† partir de descriptions en langage naturel
- ‚úÖ Utiliser Google Colab pour l'entra√Ænement et l'inf√©rence (GPU gratuit)
- ‚úÖ Ex√©cuter localement sur CPU ou GPU

**Mod√®le utilis√©:** `meta-llama/Llama-3.2-3B-Instruct`

**Technique d'entra√Ænement:** LoRA (Low-Rank Adaptation) pour un entra√Ænement efficace avec peu de ressources

## üîß Pr√©requis

### Pour Google Colab
1. **Compte Google** avec Google Drive
2. **Token Hugging Face**
   - Cr√©ez un compte sur [Hugging Face](https://huggingface.co/)
   - Acceptez la licence pour Llama 3.2: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
   - Cr√©ez un token: https://huggingface.co/settings/tokens
   - ‚ö†Ô∏è **Important:** Le token doit avoir les permissions de lecture

3. **GPU Colab** (gratuit)
   - Runtime > Change runtime type > Hardware accelerator > GPU
   - GPU recommand√©s: T4 (gratuit), V100, A100

### Pour environnement local
```bash
# Python 3.8+
pip install torch transformers datasets accelerate peft bitsandbytes sentencepiece
```

## üåê Option 1: Google Colab (Recommand√©)

### üìù √âtape 1: Pr√©paration des donn√©es

#### Option A: Cloner le repository
```python
# Dans Colab
!git clone https://github.com/VOTRE_USERNAME/FORM_EDITOR_LLM.git
%cd FORM_EDITOR_LLM
```

#### Option B: Uploader vos fichiers
1. Pr√©parez votre dataset localement avec `prepare_dataset.py`
2. Uploadez `training_dataset.jsonl` dans Colab

### üèãÔ∏è √âtape 2: Entra√Ænement

1. **Ouvrez le notebook d'entra√Ænement**
   - Uploadez `train_colab.ipynb` dans Google Colab
   - Ou ouvrez directement depuis GitHub: [Lien √† ajouter]

2. **Configuration**
   ```python
   # Dans le notebook
   # Runtime > Change runtime type > GPU
   ```

3. **Ex√©cutez les cellules dans l'ordre**
   - Installation des d√©pendances (2-3 min)
   - Montage de Google Drive
   - Authentification Hugging Face (entrez votre token)
   - Upload du dataset
   - Entra√Ænement (1-3 heures selon GPU)
   - Sauvegarde sur Google Drive

4. **Temps estim√©s**
   - T4 (gratuit): ~2-3 heures
   - V100: ~1-1.5 heures
   - A100: ~45-60 minutes

### üéØ √âtape 3: G√©n√©ration de formulaires

1. **Ouvrez le notebook d'inf√©rence**
   - Uploadez `inference_colab.ipynb` dans Google Colab

2. **Ex√©cutez les cellules**
   - Installation des d√©pendances
   - Montage de Google Drive
   - Chargement du mod√®le entra√Æn√©
   - G√©n√©ration de formulaires

3. **Exemples d'utilisation**
   ```python
   # G√©n√©ration simple
   form = generate_form(
       "Cr√©e un formulaire d'inscription avec nom, pr√©nom, email et t√©l√©phone"
   )

   # G√©n√©ration avec param√®tres
   form = generate_form(
       "Cr√©e un formulaire de contact",
       temperature=0.7,  # Cr√©ativit√©
       top_p=0.9,        # Diversit√©
       max_new_tokens=1024
   )
   ```

### üíæ Sauvegarde et t√©l√©chargement

**Le mod√®le est automatiquement sauvegard√© sur Google Drive:**
```
/content/drive/MyDrive/llama3-form-generator/
```

**Pour t√©l√©charger localement:**
```python
# Dans Colab
!zip -r llama3-form-generator.zip /content/drive/MyDrive/llama3-form-generator
from google.colab import files
files.download('llama3-form-generator.zip')
```

## üíª Option 2: Environnement local

### Installation

```bash
# Cloner le repository
git clone https://github.com/VOTRE_USERNAME/FORM_EDITOR_LLM.git
cd FORM_EDITOR_LLM

# Installer les d√©pendances
pip install -r requirements.txt

# Authentification Hugging Face
huggingface-cli login
```

### Pr√©paration des donn√©es

```bash
# G√©n√©rer le dataset
python prepare_dataset.py

# V√©rifier que training_dataset.jsonl est cr√©√©
ls -lh training_dataset.jsonl
```

### Entra√Ænement

```bash
# Lancer l'entra√Ænement
python train_model.py

# Le mod√®le sera sauvegard√© dans ./llama3-form-generator/
```

**Configuration syst√®me recommand√©e:**
- GPU: NVIDIA avec 16+ GB VRAM (RTX 3090, RTX 4090, A100)
- RAM: 32+ GB
- Stockage: 20+ GB libre

**Sans GPU:**
- L'entra√Ænement fonctionnera mais sera tr√®s lent (10-20x plus lent)
- Pr√©f√©rez Google Colab avec GPU gratuit

### G√©n√©ration

```python
from generate_form import FormGenerator

# Initialiser
generator = FormGenerator()

# G√©n√©rer
form = generator.generate_form(
    "Cr√©e un formulaire d'inscription avec nom, email et t√©l√©phone"
)

print(form)
```

## üìÅ Structure du projet

```
FORM_EDITOR_LLM/
‚îú‚îÄ‚îÄ train_model.py              # Script d'entra√Ænement (local + Colab)
‚îú‚îÄ‚îÄ generate_form.py            # G√©n√©rateur de formulaires (local + Colab)
‚îú‚îÄ‚îÄ prepare_dataset.py          # Pr√©paration du dataset
‚îú‚îÄ‚îÄ train_colab.ipynb          # üìì Notebook Colab pour l'entra√Ænement
‚îú‚îÄ‚îÄ inference_colab.ipynb      # üìì Notebook Colab pour l'inf√©rence
‚îú‚îÄ‚îÄ README_LLAMA3_COLAB.md     # Ce fichier
‚îú‚îÄ‚îÄ form_structure.json         # Structures de formulaires existantes
‚îú‚îÄ‚îÄ training_dataset.jsonl      # Dataset d'entra√Ænement (g√©n√©r√©)
‚îî‚îÄ‚îÄ llama3-form-generator/     # Mod√®le entra√Æn√© (apr√®s training)
    ‚îú‚îÄ‚îÄ adapter_config.json
    ‚îú‚îÄ‚îÄ adapter_model.bin
    ‚îú‚îÄ‚îÄ tokenizer_config.json
    ‚îî‚îÄ‚îÄ ...
```

## üéì Guide d√©taill√©

### Comprendre les param√®tres d'entra√Ænement

```python
# Dans train_model.py
TrainingArguments(
    num_train_epochs=3,              # Nombre de passages sur le dataset
    per_device_train_batch_size=2,   # Taille du batch (ajuster selon GPU)
    gradient_accumulation_steps=4,    # Accumulation de gradient
    learning_rate=2e-4,              # Taux d'apprentissage
    fp16=True,                       # Pr√©cision mixte (√©conomise la m√©moire)
)
```

**Ajustements selon votre GPU:**

| GPU | VRAM | batch_size | gradient_accumulation |
|-----|------|------------|-----------------------|
| T4 | 16GB | 1-2 | 8 |
| V100 | 32GB | 2-4 | 4 |
| A100 | 40GB | 4-8 | 2 |

### Configuration LoRA

```python
LoraConfig(
    r=16,              # Rang de la d√©composition (plus grand = plus de param√®tres)
    lora_alpha=32,     # Scaling factor (g√©n√©ralement 2*r)
    target_modules=[   # Modules √† adapter
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05, # Dropout pour r√©gularisation
)
```

**Param√®tres entra√Ænables:** ~21M param√®tres (0.7% du mod√®le complet)

### Format de prompt Llama 3.2 Instruct

```python
prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Tu es un assistant sp√©cialis√© dans la g√©n√©ration de structures de formulaires JSON.<|eot_id|><|start_header_id|>user<|end_header_id|>

{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{output}<|eot_id|>"""
```

‚ö†Ô∏è **Important:** Respectez exactement ce format pour de meilleurs r√©sultats.

### Param√®tres de g√©n√©ration

```python
generate_form(
    description="...",
    max_new_tokens=1024,  # Longueur de la g√©n√©ration
    temperature=0.7,      # Cr√©ativit√© (0.0-1.0)
    top_p=0.9,           # Diversit√© (0.0-1.0)
)
```

**Recommandations:**
- **temperature=0.3-0.5**: R√©sultats d√©terministes et coh√©rents
- **temperature=0.7-0.8**: Bon √©quilibre cr√©ativit√©/coh√©rence
- **temperature=0.9-1.0**: Tr√®s cr√©atif mais moins pr√©visible

## ‚ùì FAQ et d√©pannage

### Erreurs courantes

#### ‚ùå "RuntimeError: CUDA out of memory"
**Solutions:**
```python
# 1. R√©duire le batch size
per_device_train_batch_size=1

# 2. Augmenter gradient_accumulation_steps
gradient_accumulation_steps=8

# 3. Activer gradient checkpointing (d√©j√† activ√© par d√©faut)
gradient_checkpointing=True

# 4. R√©duire max_length
max_length=1024  # au lieu de 2048
```

#### ‚ùå "Token is not valid" (Hugging Face)
1. V√©rifiez que vous avez accept√© la licence Llama 3.2
2. Cr√©ez un nouveau token avec permissions de lecture
3. Reconnectez-vous: `huggingface-cli login`

#### ‚ùå "JSONDecodeError" lors de la g√©n√©ration
**Solutions:**
```python
# 1. R√©duire la temp√©rature
temperature=0.3

# 2. Augmenter max_new_tokens
max_new_tokens=2048

# 3. V√©rifier le format du prompt

# 4. Am√©liorer le dataset d'entra√Ænement
```

#### ‚ùå Le mod√®le g√©n√®re toujours la m√™me chose
**Solutions:**
```python
# Augmenter la temp√©rature
temperature=0.8

# Augmenter top_p
top_p=0.95

# Activer do_sample
do_sample=True
```

### Performances

**Entra√Ænement (3 epochs, 500 exemples):**
- T4 (16GB): ~2-3h
- V100 (32GB): ~1-1.5h
- A100 (40GB): ~45-60min
- CPU: ~20-30h (non recommand√©)

**Inf√©rence (g√©n√©ration d'un formulaire):**
- GPU: 2-5 secondes
- CPU: 10-30 secondes

### Optimisations

#### Pour acc√©l√©rer l'entra√Ænement:
```python
# Utiliser moins d'epochs
num_train_epochs=1

# Augmenter le batch size (si m√©moire suffisante)
per_device_train_batch_size=4

# R√©duire les eval_steps
eval_steps=100  # au lieu de 50
```

#### Pour am√©liorer la qualit√©:
```python
# Plus d'epochs
num_train_epochs=5

# Plus de donn√©es d'entra√Ænement
# Modifier prepare_dataset.py pour g√©n√©rer plus d'exemples

# Augmenter le rang LoRA
r=32  # au lieu de 16
lora_alpha=64
```

## üîó Ressources utiles

- [Documentation Llama 3.2](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
- [Guide LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)
- [Documentation Transformers](https://huggingface.co/docs/transformers/)
- [Google Colab GPU](https://colab.research.google.com/notebooks/gpu.ipynb)
- [Hugging Face Hub](https://huggingface.co/docs/hub/index)

## üìä Comparaison avec TinyLlama

| Caract√©ristique | TinyLlama 1.1B | Llama 3.2 3B |
|----------------|----------------|--------------|
| Param√®tres | 1.1B | 3B |
| Qualit√© | Basique | Excellente |
| Vitesse training | Rapide | Moyen |
| M√©moire GPU | 6-8 GB | 12-16 GB |
| Contexte | 2048 | 8192 |
| Format prompt | Simple | Instruct |

**Recommandation:** Utilisez Llama 3.2 3B pour de meilleurs r√©sultats. La diff√©rence de qualit√© justifie largement les ressources suppl√©mentaires.

## üéâ Prochaines √©tapes

Apr√®s avoir entra√Æn√© votre mod√®le:

1. **Testez diff√©rents types de formulaires**
   - Formulaires d'inscription
   - Formulaires de contact
   - Formulaires de commande
   - Formulaires de feedback

2. **Exp√©rimentez avec les param√®tres**
   - Ajustez temperature et top_p
   - Testez diff√©rentes descriptions
   - Cr√©ez vos propres templates

3. **Int√©grez dans votre application**
   - API REST avec FastAPI
   - Application web
   - Service backend

4. **Am√©liorez le mod√®le**
   - Ajoutez plus de donn√©es d'entra√Ænement
   - Fine-tunez avec vos propres formulaires
   - Augmentez le nombre d'epochs

## üí° Astuces

### Pour de meilleurs r√©sultats:
1. ‚úÖ Soyez sp√©cifique dans vos descriptions
2. ‚úÖ Listez tous les champs n√©cessaires
3. ‚úÖ Mentionnez les types de champs (select, checkbox, etc.)
4. ‚úÖ Indiquez les champs obligatoires
5. ‚úÖ Donnez du contexte sur l'usage du formulaire

### Exemples de bonnes descriptions:
```
‚úÖ "Cr√©e un formulaire d'inscription √† une conf√©rence avec nom complet,
   email (obligatoire), entreprise, poste, r√©gime alimentaire
   (v√©g√©tarien/vegan/aucun) et questions pour les speakers"

‚úÖ "Cr√©e un formulaire de commande e-commerce avec s√©lection de produit,
   quantit√© (min 1, max 10), mode de livraison (express/standard/retrait),
   adresse de livraison compl√®te et code promo optionnel"

‚ùå "Fais un formulaire" (trop vague)
‚ùå "Formulaire inscription" (manque de d√©tails)
```

## üìù License

Ce projet utilise Llama 3.2 qui est sous licence [Llama 3.2 Community License](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct).

## ü§ù Contribution

Les contributions sont les bienvenues! N'h√©sitez pas √†:
- Signaler des bugs
- Proposer des am√©liorations
- Partager vos r√©sultats
- Ajouter des exemples

## üìß Support

Pour toute question:
- Ouvrez une issue sur GitHub
- Consultez la FAQ ci-dessus
- V√©rifiez les ressources utiles

---

**Bon entra√Ænement! üöÄ**
