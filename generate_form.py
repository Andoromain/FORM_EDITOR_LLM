# generate_form.py
# G√©n√©rateur de formulaires avec Llama 3.2 3B
# Compatible avec Google Colab et environnement local

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import os

class FormGenerator:
    def __init__(self, model_path=None):
        """
        Initialise le g√©n√©rateur de formulaires.

        Args:
            model_path: Chemin vers le mod√®le entra√Æn√©.
                       Si None, d√©tecte automatiquement (Colab ou local)
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # D√©tection automatique du chemin du mod√®le
        if model_path is None:
            is_colab = 'COLAB_GPU' in os.environ or os.path.exists('/content')
            if is_colab:
                model_path = "/content/llama3-form-generator"
                # Essayer aussi sur Drive
                if not os.path.exists(model_path):
                    model_path = "/content/drive/MyDrive/llama3-form-generator"
            else:
                model_path = "./llama3-form-generator"

        self.model_path = model_path

        print(f"Device: {self.device}")
        print(f"Chargement du mod√®le depuis: {model_path}")

        # Charger le tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # Charger le mod√®le
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )

        self.model.eval()
        print("‚úÖ Mod√®le charg√© avec succ√®s!")

    def generate_form(self, description: str, max_new_tokens=1024, temperature=0.7, top_p=0.9):
        """
        G√©n√®re une structure de formulaire √† partir d'une description.

        Args:
            description: Description du formulaire √† g√©n√©rer
            max_new_tokens: Nombre maximum de tokens √† g√©n√©rer
            temperature: Temp√©rature de g√©n√©ration (0.0-1.0)
            top_p: Top-p sampling

        Returns:
            dict: Structure de formulaire JSON ou dict avec raw_output en cas d'erreur
        """
        # Format de prompt pour Llama 3.2 Instruct
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Tu es un assistant sp√©cialis√© dans la g√©n√©ration de structures de formulaires JSON.<|eot_id|><|start_header_id|>user<|end_header_id|>

{description}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extraire uniquement la r√©ponse de l'assistant
        if "assistant" in generated_text:
            response = generated_text.split("assistant")[1].strip()
        else:
            response = generated_text.strip()

        # Nettoyer la r√©ponse (retirer les balises EOT si pr√©sentes)
        response = response.replace("<|eot_id|>", "").strip()

        # Parser le JSON
        try:
            form_structure = json.loads(response)
            return form_structure
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Erreur de parsing JSON: {e}")
            print(f"R√©ponse brute: {response[:200]}...")
            return {"raw_output": response, "error": str(e)}

    def generate_from_template(self, form_type: str, fields: list):
        """
        G√©n√®re un formulaire √† partir d'un template.

        Args:
            form_type: Type de formulaire (ex: "contact", "inscription")
            fields: Liste des champs requis

        Returns:
            dict: Structure de formulaire JSON
        """
        description = f"Cr√©e un formulaire de type {form_type} avec les champs suivants : "
        description += ", ".join(fields)

        return self.generate_form(description)

    def generate_batch(self, descriptions: list, **kwargs):
        """
        G√©n√®re plusieurs formulaires en batch.

        Args:
            descriptions: Liste de descriptions
            **kwargs: Arguments pour generate_form

        Returns:
            list: Liste de structures de formulaires
        """
        results = []
        for desc in descriptions:
            result = self.generate_form(desc, **kwargs)
            results.append(result)
        return results

# Exemple d'utilisation
if __name__ == "__main__":
    print("=" * 60)
    print("G√©n√©rateur de Formulaires - Llama 3.2 3B")
    print("=" * 60)

    # Initialiser le g√©n√©rateur
    generator = FormGenerator()

    print("\n" + "=" * 60)
    print("Exemple 1: Description libre")
    print("=" * 60)

    # Exemple 1: Description libre
    form1 = generator.generate_form(
        "Cr√©e un formulaire d'inscription avec nom, pr√©nom, email, t√©l√©phone et adresse"
    )
    print("\nüìã R√©sultat:")
    print(json.dumps(form1, indent=2, ensure_ascii=False))

    print("\n" + "=" * 60)
    print("Exemple 2: Template")
    print("=" * 60)

    # Exemple 2: Template
    form2 = generator.generate_from_template(
        "contact",
        ["nom", "email", "sujet", "message"]
    )
    print("\nüìã R√©sultat:")
    print(json.dumps(form2, indent=2, ensure_ascii=False))

    print("\n" + "=" * 60)
    print("Exemple 3: Batch generation")
    print("=" * 60)

    # Exemple 3: Batch
    descriptions = [
        "Cr√©e un formulaire de commande avec produit, quantit√© et adresse",
        "Cr√©e un formulaire de feedback avec note et commentaire"
    ]
    forms = generator.generate_batch(descriptions)

    for i, form in enumerate(forms, 1):
        print(f"\nüìã Formulaire {i}:")
        print(json.dumps(form, indent=2, ensure_ascii=False))
