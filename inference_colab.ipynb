{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üéØ G√©n√©ration de Formulaires avec Llama 3.2 3B\n",
    "\n",
    "Ce notebook permet de charger votre mod√®le Llama 3.2 3B entra√Æn√© et de g√©n√©rer des structures de formulaires JSON.\n",
    "\n",
    "**Configuration:**\n",
    "- Runtime: Python 3\n",
    "- Hardware accelerator: GPU (optionnel mais recommand√©)\n",
    "\n",
    "**Pr√©requis:**\n",
    "- Avoir entra√Æn√© le mod√®le avec `train_colab.ipynb`\n",
    "- Le mod√®le doit √™tre disponible sur Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üìã √âtape 1: Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques\n",
    "!pip install -q -U transformers accelerate peft sentencepiece\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# V√©rifier le mat√©riel disponible\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CPU mode - La g√©n√©ration sera plus lente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount"
   },
   "source": [
    "## üíæ √âtape 2: Monter Google Drive\n",
    "\n",
    "Pour acc√©der au mod√®le sauvegard√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# V√©rifier que le mod√®le existe\n",
    "import os\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/llama3-form-generator\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"‚úÖ Mod√®le trouv√©: {model_path}\")\n",
    "    # Lister les fichiers\n",
    "    files = os.listdir(model_path)\n",
    "    print(f\"\\nFichiers: {', '.join(files[:5])}...\")\n",
    "else:\n",
    "    print(f\"‚ùå Mod√®le non trouv√© √†: {model_path}\")\n",
    "    print(\"\\nAssurez-vous d'avoir:\")\n",
    "    print(\"1. Entra√Æn√© le mod√®le avec train_colab.ipynb\")\n",
    "    print(\"2. Sauvegard√© le mod√®le sur Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load"
   },
   "source": [
    "## üîÑ √âtape 3: Charger le mod√®le\n",
    "\n",
    "Chargement du mod√®le entra√Æn√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Chemin du mod√®le\n",
    "model_path = \"/content/drive/MyDrive/llama3-form-generator\"\n",
    "\n",
    "print(\"üì• Chargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"üì• Chargement du mod√®le...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úÖ Mod√®le charg√© sur {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define"
   },
   "source": [
    "## üõ†Ô∏è √âtape 4: D√©finir la fonction de g√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_func"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_form(description, max_new_tokens=1024, temperature=0.7, top_p=0.9, verbose=True):\n",
    "    \"\"\"\n",
    "    G√©n√®re une structure de formulaire √† partir d'une description.\n",
    "\n",
    "    Args:\n",
    "        description: Description du formulaire\n",
    "        max_new_tokens: Nombre maximum de tokens √† g√©n√©rer\n",
    "        temperature: Temp√©rature (0.0-1.0)\n",
    "        top_p: Top-p sampling\n",
    "        verbose: Afficher les d√©tails\n",
    "\n",
    "    Returns:\n",
    "        dict: Structure de formulaire JSON\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"üìù Description: {description}\\n\")\n",
    "\n",
    "    # Format de prompt pour Llama 3.2 Instruct\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Tu es un assistant sp√©cialis√© dans la g√©n√©ration de structures de formulaires JSON.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{description}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"üîÑ G√©n√©ration en cours...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extraire la r√©ponse\n",
    "    if \"assistant\" in generated_text:\n",
    "        response = generated_text.split(\"assistant\")[1].strip()\n",
    "    else:\n",
    "        response = generated_text.strip()\n",
    "\n",
    "    response = response.replace(\"<|eot_id|>\", \"\").strip()\n",
    "\n",
    "    # Parser le JSON\n",
    "    try:\n",
    "        form_structure = json.loads(response)\n",
    "        if verbose:\n",
    "            print(\"‚úÖ G√©n√©ration r√©ussie!\\n\")\n",
    "        return form_structure\n",
    "    except json.JSONDecodeError as e:\n",
    "        if verbose:\n",
    "            print(f\"‚ö†Ô∏è Erreur de parsing JSON: {e}\")\n",
    "            print(f\"R√©ponse brute: {response[:200]}...\\n\")\n",
    "        return {\"raw_output\": response, \"error\": str(e)}\n",
    "\n",
    "print(\"‚úÖ Fonction de g√©n√©ration d√©finie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examples"
   },
   "source": [
    "## üé® √âtape 5: Exemples de g√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example1"
   },
   "outputs": [],
   "source": [
    "# Exemple 1: Formulaire d'inscription\n",
    "print(\"=\" * 60)\n",
    "print(\"EXEMPLE 1: Formulaire d'inscription\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "form1 = generate_form(\n",
    "    \"Cr√©e un formulaire d'inscription avec nom, pr√©nom, email, t√©l√©phone et adresse\"\n",
    ")\n",
    "\n",
    "print(\"üìã R√©sultat:\")\n",
    "print(json.dumps(form1, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example2"
   },
   "outputs": [],
   "source": [
    "# Exemple 2: Formulaire de contact\n",
    "print(\"=\" * 60)\n",
    "print(\"EXEMPLE 2: Formulaire de contact\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "form2 = generate_form(\n",
    "    \"Cr√©e un formulaire de contact avec nom, email, sujet et message\"\n",
    ")\n",
    "\n",
    "print(\"üìã R√©sultat:\")\n",
    "print(json.dumps(form2, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example3"
   },
   "outputs": [],
   "source": [
    "# Exemple 3: Formulaire de commande\n",
    "print(\"=\" * 60)\n",
    "print(\"EXEMPLE 3: Formulaire de commande\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "form3 = generate_form(\n",
    "    \"Cr√©e un formulaire de commande avec produit, quantit√©, prix et adresse de livraison\"\n",
    ")\n",
    "\n",
    "print(\"üìã R√©sultat:\")\n",
    "print(json.dumps(form3, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom"
   },
   "source": [
    "## ‚úèÔ∏è √âtape 6: G√©n√©ration personnalis√©e\n",
    "\n",
    "Modifiez la description ci-dessous pour g√©n√©rer votre propre formulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_gen"
   },
   "outputs": [],
   "source": [
    "# üéØ Personnalisez votre description ici\n",
    "custom_description = \"Cr√©e un formulaire d'inscription √† un √©v√©nement avec nom, pr√©nom, email, entreprise et commentaires\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"G√âN√âRATION PERSONNALIS√âE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "custom_form = generate_form(\n",
    "    custom_description,\n",
    "    temperature=0.7,  # Ajustez la cr√©ativit√© (0.0-1.0)\n",
    "    top_p=0.9,        # Ajustez la diversit√© (0.0-1.0)\n",
    ")\n",
    "\n",
    "print(\"üìã R√©sultat:\")\n",
    "print(json.dumps(custom_form, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch"
   },
   "source": [
    "## üì¶ √âtape 7: G√©n√©ration en batch\n",
    "\n",
    "G√©n√©rez plusieurs formulaires d'un coup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_gen"
   },
   "outputs": [],
   "source": [
    "# Liste de descriptions\n",
    "descriptions = [\n",
    "    \"Cr√©e un formulaire de feedback avec note de 1 √† 5 et commentaire\",\n",
    "    \"Cr√©e un formulaire de candidature avec nom, CV et lettre de motivation\",\n",
    "    \"Cr√©e un formulaire de r√©servation avec date, heure et nombre de personnes\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"G√âN√âRATION EN BATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_forms = []\n",
    "for i, desc in enumerate(descriptions, 1):\n",
    "    print(f\"\\nüìù Formulaire {i}/{len(descriptions)}\")\n",
    "    form = generate_form(desc, verbose=False)\n",
    "    batch_forms.append(form)\n",
    "    print(f\"‚úÖ G√©n√©r√©!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"R√âSULTATS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, form in enumerate(batch_forms, 1):\n",
    "    print(f\"\\nüìã Formulaire {i}:\")\n",
    "    print(json.dumps(form, indent=2, ensure_ascii=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export"
   },
   "source": [
    "## üíæ √âtape 8: Exporter les r√©sultats\n",
    "\n",
    "Sauvegardez vos formulaires g√©n√©r√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# Sauvegarder dans un fichier JSON\n",
    "output_file = \"generated_forms.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(batch_forms, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ R√©sultats sauvegard√©s dans: {output_file}\")\n",
    "\n",
    "# T√©l√©charger le fichier\n",
    "from google.colab import files\n",
    "files.download(output_file)\n",
    "\n",
    "print(\"üì• T√©l√©chargement lanc√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_drive"
   },
   "outputs": [],
   "source": [
    "# Sauvegarder sur Google Drive\n",
    "import shutil\n",
    "\n",
    "drive_output = \"/content/drive/MyDrive/generated_forms.json\"\n",
    "shutil.copy(output_file, drive_output)\n",
    "\n",
    "print(f\"‚úÖ R√©sultats sauvegard√©s sur Drive: {drive_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive"
   },
   "source": [
    "## üéÆ √âtape 9: Mode interactif\n",
    "\n",
    "G√©n√©rez des formulaires interactivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive_mode"
   },
   "outputs": [],
   "source": [
    "# Mode interactif\n",
    "while True:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    description = input(\"üìù D√©crivez le formulaire (ou 'quit' pour quitter): \")\n",
    "\n",
    "    if description.lower() in ['quit', 'q', 'exit']:\n",
    "        print(\"üëã Au revoir!\")\n",
    "        break\n",
    "\n",
    "    if not description.strip():\n",
    "        print(\"‚ö†Ô∏è Description vide!\")\n",
    "        continue\n",
    "\n",
    "    form = generate_form(description)\n",
    "    print(\"\\nüìã R√©sultat:\")\n",
    "    print(json.dumps(form, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tips"
   },
   "source": [
    "## üí° Conseils et astuces\n",
    "\n",
    "### Param√®tres de g√©n√©ration:\n",
    "- **temperature** (0.0-1.0): Contr√¥le la cr√©ativit√©\n",
    "  - Basse (0.1-0.3): R√©sultats plus d√©terministes\n",
    "  - Moyenne (0.5-0.7): Bon √©quilibre\n",
    "  - Haute (0.8-1.0): Plus cr√©atif mais moins pr√©visible\n",
    "\n",
    "- **top_p** (0.0-1.0): Contr√¥le la diversit√©\n",
    "  - Valeur recommand√©e: 0.9\n",
    "\n",
    "- **max_new_tokens**: Nombre de tokens √† g√©n√©rer\n",
    "  - Formulaire simple: 512\n",
    "  - Formulaire complexe: 1024-2048\n",
    "\n",
    "### Conseils pour de meilleurs r√©sultats:\n",
    "1. Soyez sp√©cifique dans vos descriptions\n",
    "2. Listez tous les champs n√©cessaires\n",
    "3. Mentionnez les types de champs si n√©cessaire (select, checkbox, etc.)\n",
    "4. Indiquez les champs obligatoires si pertinent\n",
    "5. Ajustez la temp√©rature selon vos besoins\n",
    "\n",
    "### Exemples de descriptions efficaces:\n",
    "- ‚úÖ \"Cr√©e un formulaire d'inscription avec nom (texte obligatoire), email (obligatoire), t√©l√©phone (optionnel) et pays (menu d√©roulant)\"\n",
    "- ‚úÖ \"Cr√©e un formulaire de commande avec choix de produit, quantit√©, mode de livraison (express/standard) et adresse compl√®te\"\n",
    "- ‚ùå \"Fais un formulaire\" (trop vague)\n",
    "\n",
    "### D√©pannage:\n",
    "- Si le JSON est invalide, essayez de diminuer la temp√©rature\n",
    "- Si les r√©sultats sont trop r√©p√©titifs, augmentez la temp√©rature\n",
    "- Si le mod√®le est lent, v√©rifiez que vous utilisez un GPU\n",
    "- En cas d'erreur de m√©moire, r√©duisez max_new_tokens"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
